{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yavuzuzun/projects/blob/main/Lab1_ECE442.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIttxWZg5D8S"
      },
      "source": [
        "# This notebook is a Homework prepared for thr ECE 442 Network Science Analytics class taught by Gonzalo Mateos during Spring 2023\n",
        "## Manipulating network graphs, introduction to NetworkX and PyTorch Geometric\n",
        "\n",
        "In this first laboratory we will work with a real dataset, generate a network graph and analyze it using the Python package **[NetworkX](https://networkx.org/)**. We will also introduce **[pandas](https://pandas.pydata.org/)**, an excellent library to load and process datasets efficiently. A third goal of this assigment is to start familiarizing ourselves with **[PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)**, a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to network data.\n",
        "\n",
        "To this end, we will study the email graph of the Enron corporation. Emails exchanged among several Enron employees in the period between November 1998 and June 2002 were made publicly available during the federal investigation; for additional details about the Enron scandal see https://en.wikipedia.org/wiki/Enron_scandal.  The completed dataset can be accessed from http://www.cs.cmu.edu/~enron/. Here we will use a smaller and curated version of the email corpus (for instance, with the email body removed), which can be obtained from http://cis.jhu.edu/~parky/Enron/enron.html. \n",
        "\n",
        "For those of you who have never worked with the aforementioned libraries, we hope this laboratory will provide a useful first exposure and bring you up to speed with what you will need for the rest of the course. We ask you upload to Gradescope the answers to all the questions that follow in a report submitted as a single pdf file. You are welcome to explore and play with the data beyond what we ask; let us know what you find!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v5d66ux0r8x"
      },
      "source": [
        "### Network graph generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljqF2gmO54y3"
      },
      "outputs": [],
      "source": [
        "# load the libraries we will use\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWGrbfmQ4qKe"
      },
      "outputs": [],
      "source": [
        "# get the dataset (see http://cis.jhu.edu/~parky/Enron/enron.html for additional details)\n",
        "!wget http://cis.jhu.edu/~parky/Enron/employees\n",
        "!wget http://cis.jhu.edu/~parky/Enron/execs.email.linesnum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UjEEZHy50qR"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "df_mails = pd.read_csv('execs.email.linesnum', names=['time','from','to'], sep=' ')\n",
        "df_employees = pd.read_csv('employees', sep='\\t', names=['mail', 'name and more'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA0VzjMw7VlS"
      },
      "source": [
        "In the variable `df_mails` we store a pandas [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) with the id of the sender (`from` column) and recepient (`to`) of an email sent at a given timestamp (`time`). In addition, the email user account and other information from the employees are stored in the dataframe  `df_employees`. You can think of a dataframe as an indexed table, but pandas offers plenty of additional functionalities, some of which we will leverage to process the data and generate the network graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaR2uNLG-auM"
      },
      "outputs": [],
      "source": [
        "# compute the dates from the timestamp (in seconds from 1/1/1970)\n",
        "df_mails['date'] = pd.to_datetime(df_mails.time, unit='s')\n",
        "\n",
        "# strangely enough there are dates from 1979. Let's remove those.\n",
        "df_mails = df_mails[df_mails.date.dt.year>1980]\n",
        "\n",
        "df_mails.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiNIuXYP8umA"
      },
      "source": [
        "### Graph construnction for the entire time horizon\n",
        "\n",
        "First we construct a network graph spanning all emails.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAmW8c7kEIb7"
      },
      "outputs": [],
      "source": [
        "# count number of emails between a pair of users\n",
        "mails_exchanged = df_mails.groupby(['from', 'to']).count().reset_index()\n",
        "mails_exchanged.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpxWsTfAE65P"
      },
      "outputs": [],
      "source": [
        "# the columns \"time\" and \"date\" have the same information, so abrbitrarily change one to \"weight\" which I will use to define edge weights\n",
        "mails_exchanged.rename(columns={'time':'weight'}, inplace=True)\n",
        "mails_exchanged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2TZkz7OGEEK"
      },
      "outputs": [],
      "source": [
        "# and here is something nice: pandas can be interfaced with networkx. \n",
        "G = nx.from_pandas_edgelist(mails_exchanged, source='from', target='to', edge_attr='weight', create_using=nx.DiGraph)\n",
        "\n",
        "# remove self loops\n",
        "G.remove_edges_from(nx.selfloop_edges(G))\n",
        "\n",
        "# generating a graph visualization is easy...\n",
        "nx.draw_networkx(G)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLXKPBT7RPx"
      },
      "outputs": [],
      "source": [
        "# ... but cannot see much, typical ball of yarn phenomena we encounter with large graphs.\n",
        "\n",
        "# so let's be a little bit more creative\n",
        "positions = nx.circular_layout(G)\n",
        "edges = G.edges()\n",
        "weights = np.array([G[u][v]['weight'] for u,v in edges])\n",
        "\n",
        "between_dict = nx.betweenness_centrality(G)\n",
        "between = np.array(list(between_dict.values()))\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "nx.draw_networkx_nodes(G, pos=positions, node_color=10*np.log(1+between/(np.min(between)+1e-9)), cmap='Blues')\n",
        "nx.draw_networkx_edges(G, alpha=0.1, width=np.log10(weights+1), pos=positions)\n",
        "nx.draw_networkx_labels(G, pos=positions, font_color='black')\n",
        "plt.title('Network graph of emails exchanged during the whole time period.\\n Edge width is proportional to the number of emails exchanged (log scale).\\n \\\n",
        "  Vertex color intensity is proportional to its betweeness centrality (log scale).', fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVkzFblZaH2x"
      },
      "source": [
        "### Interfacing NetworkX with NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiT906OPXkER"
      },
      "outputs": [],
      "source": [
        "# in addition to interfacing with pandas, NetworkX can work with NumPy and matrices\n",
        "\n",
        "# for instance, obtaining the adjacency matrix is as simple as this\n",
        "G_np = nx.to_numpy_array(G,nodelist=range(G.number_of_nodes()))\n",
        "# we plot it using seaborn\n",
        "sns.heatmap(np.log10(G_np+1), cmap='Greys')\n",
        "plt.show()\n",
        "# or we can exclusively focus on the connecitivity pattern...\n",
        "sns.heatmap(G_np>0, cmap='Greys')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sscNxoyVx4K"
      },
      "source": [
        "## Network analysis\n",
        "\n",
        "Now you should use the Networkx or NumPy APIs to compute various summary statistics of the network graph `G(V,E)`: \n",
        "\n",
        "\n",
        "1.   Number of directed edges (arcs) in the network, i.e., the number of unique ordered pairs $(u,v)\\in E$,\n",
        "where $u,v\\in V$.\n",
        "2.   Number of undirected edges in the network, i.e., the number of unique unordered pairs $(u,v)\\in E$,\n",
        "where $u,v\\in V$. (This means that if at least one of $(u,v)\\in E$ or $(v,u)\\in E$, you count the pair as a single undirected edge.)\n",
        "3.   Number of mutual arcs in the network, i.e., the number of pairs $(u,v)$, where $\\{(u,v),(v,u)\\}\\subseteq E$\n",
        "and $u,v\\in V$. (This means that if both $(u,v)\\in E$ and $(v,u)\\in E$, you count the pair as a mutual arc.)\n",
        "4.   Number of nodes with $d_v^{\\text{in}}=0$, and list the corresponding employee names.\n",
        "5.   Number of nodes with $d_v^{\\text{out}}=0$, and list the corresponding employee names.\n",
        "6.   Number of employees that have been contacted by 30 or more employees. Generate a new graph visualization and: (i) color these nodes in red; (ii) label these nodes with the corresponding employee names.  \n",
        "7.   Number of employees that have contacted 30 or more employees. Generate a new graph visualization and: (i) color these nodes in red; (ii) label these nodes with the corresponding employee names. \n",
        "8.   Histogram of vertex degrees (separate $d_v^{\\text{in}}$ and $d_v^{\\text{out}}$). You can for instance use the histplot tool in seaborn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. G is generated with diGraph. We may directly ask for the unique set."
      ],
      "metadata": {
        "id": "d0Om_FBwwSSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('There are' + ' ' + str(len(G.edges())) + ' ' + 'unique directed edges in the graph')"
      ],
      "metadata": {
        "id": "P734jogVwj-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. We can get the unique undirected edges by generating an undirected graph."
      ],
      "metadata": {
        "id": "mvGDAiWPwSee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G_unDir = nx.from_pandas_edgelist(mails_exchanged, source='from', target='to', edge_attr='weight', create_using=nx.Graph)\n",
        "print('There are' + ' ' + str(len(G_unDir.edges())) + ' ' + 'unique undirected edges in the graph')\n"
      ],
      "metadata": {
        "id": "QuM5cJZfwpHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3."
      ],
      "metadata": {
        "id": "9I-3KJD0wSm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('To prevent double counting, one need to substract the number of the edges\\n\\\n",
        "working both way in a directed graph to find the number of the undirected edges.\\n\\\n",
        "So the difference between the count of unique directed and undirected edges gives\\n\\\n",
        "the number of the mutual arcs, which is' + ' ' + str(len(G.edges())-len(G_unDir.edges())) + '.')\n"
      ],
      "metadata": {
        "id": "WwC-kbg7wuML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In edges for a node is stored in the corresponding column. We can find the nodes without an edge by finding the zeros of the sum of the adjacency matrix along the first dimension."
      ],
      "metadata": {
        "id": "WfZY0Th0wSto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nodes without an incoming edge\n",
        "d_in = np.sum(G_np,0)\n",
        "d_in_isolated = np.where(d_in == 0)[0]\n",
        "d_in_isolated \n"
      ],
      "metadata": {
        "id": "aImPiuREwx_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corresponding names of the nodes are shown below\n",
        "df_employees.loc[d_in_isolated,\"name and more\"]\n"
      ],
      "metadata": {
        "id": "eBUnzSWtw1bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5."
      ],
      "metadata": {
        "id": "Y9rs2jAwwSxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nodes without an out edge\n",
        "d_out = np.sum(G_np,1)\n",
        "d_out_isolated = np.where(d_out == 0)[0]\n",
        "d_out_isolated \n"
      ],
      "metadata": {
        "id": "QTY8EXOZw4GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corresponding names of the nodes are shown below\n",
        "df_employees.loc[d_out_isolated,\"name and more\"]\n"
      ],
      "metadata": {
        "id": "SHJXhvqqw57l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. We can find the nodes with a high incoming edge with thresholding."
      ],
      "metadata": {
        "id": "2CSkwcVcwS0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_in_dense = np.where(d_in >= 30)[0]"
      ],
      "metadata": {
        "id": "qkH5O1C_xIzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_map = []\n",
        "labels = {}\n",
        "ii = 0\n",
        "for node in range(len(G.nodes())):\n",
        "    if node in d_in_dense:\n",
        "        color_map.append('red')\n",
        "        labels[ii] = df_employees.loc[node,\"name and more\"]\n",
        "        ii += 1\n",
        "    else: \n",
        "        color_map.append('blue')\n",
        "        labels[ii] = \"\"\n",
        "        ii += 1\n",
        "\n",
        "pos = nx.circular_layout(G)\n",
        "edges = G.edges()\n",
        "weights = np.array([G[u][v]['weight'] for u,v in edges])\n",
        "\n",
        "between_dict = nx.betweenness_centrality(G)\n",
        "between = np.array(list(between_dict.values()))\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "nx.draw_networkx_nodes(G, pos=positions, node_color=color_map)\n",
        "nx.draw_networkx_edges(G, alpha=0.1, width=np.log10(weights+1), pos=positions)\n",
        "nx.draw_networkx_labels(G, pos, labels, font_color='black')\n",
        "plt.title('Network graph of emails exchanged during the whole time period.\\n \\\n",
        "  Vertex color is red for d_in >= 30, and blue for d_in < 30.', fontsize=18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nax1QEr2xCKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7."
      ],
      "metadata": {
        "id": "phKMJyDOwS3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_out_dense = np.where(d_out >= 30)[0]"
      ],
      "metadata": {
        "id": "7h8kd5vMxMcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_map = []\n",
        "labels = {}\n",
        "ii = 0\n",
        "for node in range(len(G.nodes())):\n",
        "    if node in d_out_dense:\n",
        "        color_map.append('red')\n",
        "        labels[ii] = df_employees.loc[node,\"name and more\"]\n",
        "        ii += 1\n",
        "    else: \n",
        "        color_map.append('blue')\n",
        "        labels[ii] = \"\"\n",
        "        ii += 1\n",
        "\n",
        "pos = nx.circular_layout(G)\n",
        "edges = G.edges()\n",
        "weights = np.array([G[u][v]['weight'] for u,v in edges])\n",
        "\n",
        "between_dict = nx.betweenness_centrality(G)\n",
        "between = np.array(list(between_dict.values()))\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "nx.draw_networkx_nodes(G, pos=positions, node_color=color_map)\n",
        "nx.draw_networkx_edges(G, alpha=0.1, width=np.log10(weights+1), pos=positions)\n",
        "nx.draw_networkx_labels(G, pos, labels, font_color='black')\n",
        "plt.title('Network graph of emails exchanged during the whole time period.\\n \\\n",
        "  Vertex color is red for d_in >= 30, and blue for d_in < 30.', fontsize=18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yppONUd3xM4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Histogram of vertex degrees (separate ùëëinùë£ and ùëëoutùë£ ). You can for instance use the histplot tool in seaborn."
      ],
      "metadata": {
        "id": "l_WLWL3qwS6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(np.sum(G_np,0))\n",
        "plt.title('Distribution of d_in')\n",
        "plt.xlabel('in_edges')\n",
        "plt.ylabel('count')\n"
      ],
      "metadata": {
        "id": "kR4LNZJ6xU2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(np.sum(G_np,1))\n",
        "plt.title('Distribution of d_out')\n",
        "plt.xlabel('out_edges')\n",
        "plt.ylabel('count')\n"
      ],
      "metadata": {
        "id": "deCy-LqoxaEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZn1nm5CagbZ"
      },
      "source": [
        "## Dynamic (temporal) network analysis\n",
        "\n",
        "So far we have examined the entire dataset and ignored its temporal dimension. To bridge this gap, in this section we will carry out a simple dynamic network analysis to study how the graph changes across time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADMP8EEVXTZq"
      },
      "outputs": [],
      "source": [
        "# let's cluster emails per week, so we first check to which week a given email corresponds to and then we add it to df_mails\n",
        "df_mails['week'] = df_mails.date.dt.to_period('W')\n",
        "print(df_mails.head())\n",
        "\n",
        "# per week aggregation. This generates a GroupBy object over which we can iterate, and contains all data for each week\n",
        "grouped_week = df_mails.groupby('week')\n",
        "# list that will contain the weekly network graphs\n",
        "graphs = []\n",
        "# list that will contain the weeeks themselves. Come be used to identify timestamps down the road. \n",
        "weeks = []\n",
        "\n",
        "for week_id, mails_group in grouped_week:\n",
        "    # we basically repeated what we did for the entire graph, but on a per week basis. \n",
        "    # we will be storing the weekly graphs in a list. Arguably not the most efficient approach, but the dataset is not that large\n",
        "\n",
        "    # count number of emails between a pair of users this week\n",
        "    mails_exchanged = mails_group.groupby(['from', 'to']).count().reset_index()\n",
        "    # the columns have the same information, so abrbitrarily change one to \"weight\" which I will use to define edge weights\n",
        "    mails_exchanged.rename(columns={'week':'weight'}, inplace=True)\n",
        "    G = nx.from_pandas_edgelist(mails_exchanged, source='from', target='to', edge_attr='weight', create_using=nx.DiGraph)\n",
        "    \n",
        "    # remove self loops\n",
        "    G.remove_edges_from(nx.selfloop_edges(G))\n",
        "    \n",
        "    # add the new graph to the list\n",
        "    graphs.append(G)\n",
        "    weeks.append(week_id)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgyUT8NnsVoS"
      },
      "outputs": [],
      "source": [
        "# let's examine the temporal evolution of some simple summary statistcs\n",
        "\n",
        "num_nodes = [current_graph.number_of_nodes() for current_graph in graphs]\n",
        "num_arcs = [current_graph.number_of_edges() for current_graph in graphs]\n",
        "pd.DataFrame({'n_nodes':num_nodes, 'n_arcs':num_arcs}, index=weeks).plot(figsize=(12,6))\n",
        "plt.grid()\n",
        "plt.legend(['Number of nodes', 'Number of arcs'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuoDL6Vx3vvm"
      },
      "source": [
        "### Changes in the network graph\n",
        "9. Pick two node centrality measures of your choice (see e.g., Ch. 4 of E. Kolaczyk's book Statistical Analysis of Network Data, the [lecture slides on centrality](https://www.hajim.rochester.edu/ece/sites/gmateos/ECE442/Slides/block_3_descriptive_analysis_properties_part_c.pdf), or the [NetworkX documentation](https://networkx.org/documentation/stable/reference/algorithms/centrality.html)) and indicate who was the most central Enron employee each week according to each of these measures.  Compare your results with what you obtain for the \"entire\" graph (namely, the network constructed earlier using data for the whole time horizon). \n",
        "10. Experiment with a few graph-level summary statistics (e.g., number of nodes, edges, average degree, average clustering coefficient, or any other of your liking) and use them to identify some of the major events tied to the scandal (Figure 8 in https://arxiv.org/abs/1403.0989 has a very nice timeline that could help). Likely you should be able to spot the launch of Enron online and Stephen Cooper's ascent to the CEO role."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Here I found the most central node using betweenness centrality and eigenvalue centrality. Initially I showed the results without taking time into account. Later, showed the results for the course of the scandal."
      ],
      "metadata": {
        "id": "dGPWJGhErp-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the directed graph for . \n",
        "G_all = nx.from_pandas_edgelist(mails_exchanged, source='from', target='to', edge_attr='weight', create_using=nx.DiGraph)\n",
        "G_all.remove_edges_from(nx.selfloop_edges(G_all))"
      ],
      "metadata": {
        "id": "4DrGqlVkwTF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_all = nx.betweenness_centrality(G_all)\n",
        "# node with the maximum betweenness centrality\n",
        "max_node_bc  = max(bc_all, key=bc_all.get)\n",
        "print(df_employees.loc[max_node_bc,\"name and more\"] , ' ' , 'is the most central node according to the betweenness\\n\\\n",
        " centrality without taking the temporal part of the graph into consideration')"
      ],
      "metadata": {
        "id": "Nd-WB2MY61P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ec_all = nx.eigenvector_centrality(G_all, max_iter=10000)\n",
        "# node with the maximum betweenness centrality\n",
        "max_node_ec  = max(ec_all, key=ec_all.get)\n",
        "print(df_employees.loc[max_node_ec,\"name and more\"] , ' ' , 'is the most central node according to the eigenvalue\\n\\\n",
        " centrality without taking the temporal part of the graph into consideration')"
      ],
      "metadata": {
        "id": "rM_sU44j6i-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data frame already has a column for weeks. We can directly use it.\n",
        "# per week aggregation. This generates a GroupBy object over which we can iterate, and contains all data for each week\n",
        "grouped_week = df_mails.groupby('week')\n",
        "# list that will contain the weekly network graphs\n",
        "graphs = []\n",
        "# list that will contain the weeeks themselves. Come be used to identify timestamps down the road. \n",
        "weeks = []\n",
        "\n",
        "# create list to store central nodes for each week \n",
        "betweenness_c = []\n",
        "eigenvector_c = []\n",
        "\n",
        "for week_id, mails_group in grouped_week:\n",
        "    # we basically repeated what we did for the entire graph, but on a per week basis. \n",
        "    # we will be storing the weekly graphs in a list. Arguably not the most efficient approach, but the dataset is not that large\n",
        "\n",
        "    # count number of emails between a pair of users this week\n",
        "    mails_exchanged = mails_group.groupby(['from', 'to']).count().reset_index()\n",
        "    # the columns have the same information, so abrbitrarily change one to \"weight\" which I will use to define edge weights\n",
        "    mails_exchanged.rename(columns={'week':'weight'}, inplace=True)\n",
        "    G = nx.from_pandas_edgelist(mails_exchanged, source='from', target='to', edge_attr='weight', create_using=nx.DiGraph)\n",
        "    # remove self loops\n",
        "    G.remove_edges_from(nx.selfloop_edges(G))\n",
        "\n",
        "    bc = nx.betweenness_centrality(G)\n",
        "    # node with the maximum betweenness centrality\n",
        "    max_node_bc  = max(bc, key=bc.get)\n",
        "    print(df_employees.loc[max_node_bc,\"name and more\"] , 'is the most central node according to the betweenness\\n\\\n",
        "    centrality for week ', week_id)\n",
        "\n",
        "    ec = nx.eigenvector_centrality(G, max_iter=10000)\n",
        "    # node with the maximum betweenness centrality\n",
        "    max_node_ec  = max(ec, key=ec.get)\n",
        "    print(df_employees.loc[max_node_ec,\"name and more\"] , ' ' , 'is the most central node according to the eigenvalue\\n\\\n",
        "    centrality for week ', week_id)\n",
        "    \n",
        "    # add the new graph to the list\n",
        "    graphs.append(G)\n",
        "    weeks.append(week_id)\n",
        "    betweenness_c.append(max_node_bc)\n",
        "    eigenvector_c.append(max_node_ec)"
      ],
      "metadata": {
        "id": "3Bj8XmtA6rh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall Chris Germany is the most central node according to betweenness and eigenvector centralities. However, temporal part indicates that Chris Germany is become the most central node only for the last one and a half month."
      ],
      "metadata": {
        "id": "pyskOIX3AS9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Number of nodes, edges, average degree, average clustering coefficient as an indicator of changes over the network."
      ],
      "metadata": {
        "id": "RgOYU4ABBkGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colect the statistical feature of the graph for each week and plot\n",
        "\n",
        "num_nodes = [current_graph.number_of_nodes() for current_graph in graphs]\n",
        "num_arcs = [current_graph.number_of_edges() for current_graph in graphs]\n",
        "averageDegree = [current_graph.number_of_edges()/current_graph.number_of_nodes() for current_graph in graphs]\n",
        "averageClustering = [nx.average_clustering(current_graph) for current_graph in graphs]\n",
        "\n",
        "pd.DataFrame({'n_nodes':num_nodes, 'n_arcs':num_arcs}, index=weeks).plot(figsize=(12,6))\n",
        "plt.grid()\n",
        "plt.legend(['Number of nodes', 'Number of arcs'])\n",
        "plt.show()\n",
        "\n",
        "pd.DataFrame({'averageDegree' : averageDegree, 'averageClustering' : averageClustering}, index=weeks).plot(figsize=(12,6))\n",
        "plt.grid()\n",
        "plt.legend(['Average degree', 'Average Clustering'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tG1qIDyPIECY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The launch of Enron online corresponds to the fluctiations of the average clustering at the end of 1999, and Stephen Cooper's ascent to the CEO role corresponds to the plummiting of the average clustering coefficient in 2002."
      ],
      "metadata": {
        "id": "2g30P2KaOLFB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plUYGwb-oh0J"
      },
      "source": [
        "## Introduction to Pytorch Geometric (PyG)\n",
        "**[PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric)** is a Python library for deep learning on graphs, which provides the required functionatility to work with Graph Neural Networks (GNNs). The library is an extension of **[PyTorch](https://pytorch.org/)**, arguably the most widely adopted open source deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLfWs_xwoh0J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version is {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S33uGlepoh0K"
      },
      "outputs": [],
      "source": [
        "# install PyG for the working version of PyTorch\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTHkIICqoh0K"
      },
      "source": [
        "PyG includes several network datasets in the package **[torch_geometric.datasets](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets)**. In this part of the laboratory we will work with a dataset that has become a *de facto* testbed for community detection algorithms, namely [**Zachary's karate club network**](https://en.wikipedia.org/wiki/Zachary%27s_karate_club)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxjTi2vioh0K"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import KarateClub\n",
        "\n",
        "dataset = KarateClub()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRf8X-czoh0L"
      },
      "source": [
        "The dataset consists of a single network graph, each vertex has an associated vector in $\\mathbb{R}^{34}$ (a so-termed nodal *feature* vector), and nodes are partitioned in 4 classes. Let's examine some other network summary statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz2j97_foh0M"
      },
      "outputs": [],
      "source": [
        "# focus on the first time (and only) graph\n",
        "data = dataset[0]\n",
        "\n",
        "print(data)\n",
        "print('==============================================================')\n",
        "\n",
        "# network charactersitics\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average degree: {(2*data.num_edges) / data.num_nodes:.2f}')\n",
        "print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Graph has self loops: {data.has_self_loops()}')\n",
        "print(f'Graph is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB_zV8q-oh0M"
      },
      "source": [
        "A graph in PyG by an object of type [`Data`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data). Each of these objects has at least 5 attributes:\n",
        "- **`x`**: is a network-wide feature matrix associated to the vertices (that is, a matrix whose columns are the nodal feature vectors). It is an object of type [`tensor`](https://pytorch.org/docs/stable/tensors.html), torch's native type to store matrices (the equivalent to [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) in numpy).\n",
        "- **`edge_index`**: is the graph's connectivity matrix in [COO](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)) format. This format is very useful to store and work with *sparse* matrices (those having a large number of zeros, here denoting non-edges). It only stores a list of nodes connected by edges, instead of storting the whole adjacency matrix.\n",
        "- **`y`**: is a matrix of nodal labels (for the Karate club, the matrix that encodes the class membership of each vetex).\n",
        "- **`train_mask`**: binary matrix indicating the subset of vertices that are part of the training set. This will be useful down the road when we e.g.,  build and train a GNN model for node classification.\n",
        "- **`edge_attr`**: is a network-wide feature matrix associated to the edges. Since the Karate club network is unweighted, the dataset has no edge features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebs9sGvkoh0M"
      },
      "outputs": [],
      "source": [
        "print('data.x')\n",
        "print('========================================')\n",
        "print(data.x)\n",
        "print('\\ndata.edge_index')\n",
        "print('=========================================')\n",
        "print(data.edge_index.t())\n",
        "print('\\ndata.y')\n",
        "print('=========================================')\n",
        "print(data.y)\n",
        "print('\\ndata.train_mask')\n",
        "print('=========================================')\n",
        "print(data.train_mask)\n",
        "print('\\ndata.edge_attr')\n",
        "print('=========================================')\n",
        "print(data.edge_attr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLEBY1ojoh0N"
      },
      "source": [
        "PyG offers a simple interface to convert a graph into NetworkX's format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPIVsaswoh0N"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "nx.draw_networkx(G,node_color=data.y,pos=nx.spring_layout(G, seed=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKK__9ZAoh0N"
      },
      "source": [
        "## Verify properties of the graph Laplacian\n",
        "The goal of the following questions is to empirically verify a few properties of the graph Laplacian matrix. In the **optional** exercise below, you are asked to mathematically establish those properties.\n",
        "\n",
        "11. Compute the graph Laplacian matrix $\\mathbf{L}$ for Zachary's karate club network. You are encouraged to use some suitable function from the subpackage [`torch_geometric.utils`](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html).\n",
        "12. Check that $\\mathbf{L}$ has a 0 eigenvalue and verify that the vector of all ones $[1,1,\\dots,1]^\\top$ is the corresponding eigenvector. The subpackage [`torch.linalg`](https://pytorch.org/docs/stable/linalg.html) may be useful to that end.\n",
        "13. Corroborate that $\\mathbf{L}$ is a symmetric positive semidefinite matrix.\n",
        "14. Form a matrix $\\tilde{\\mathbf{B}}$ as described in Part 2 of the optional exercise below and verify that $\\mathbf{L}=\\tilde{\\mathbf{B}}\\tilde{\\mathbf{B}}^\\top$. You are encouraged to use the function [`networkx.incidence_matrix`](https://networkx.org/documentation/stable/reference/generated/networkx.linalg.graphmatrix.incidence_matrix.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Graph Laplacian"
      ],
      "metadata": {
        "id": "7UhKb54XYy2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.csgraph import laplacian\n",
        "aa = nx.to_numpy_array(G)\n",
        "L = laplacian(aa)\n",
        "L"
      ],
      "metadata": {
        "id": "4PwF_icdqzEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L[0]"
      ],
      "metadata": {
        "id": "VjoPvCI684w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12."
      ],
      "metadata": {
        "id": "WzxzanDh_8xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.linalg import eig\n",
        "w, v = eig(L)"
      ],
      "metadata": {
        "id": "3QSl5naIh9Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w # Eigenvalue array"
      ],
      "metadata": {
        "id": "qxRlt4P_4ioQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only eigenvalue that is negative is on the order of -e15. So, we can assume that it is zero.\n",
        "# The corresponding eigenvector is\n",
        "eigMin = v[:,np.where(w==w.min())]\n",
        "eigMin = eigMin/np.mean(eigMin)\n",
        "np.transpose(eigMin)"
      ],
      "metadata": {
        "id": "LEsApdMs4qF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirms that the corresponding eigenvector is array of ones."
      ],
      "metadata": {
        "id": "Ite--Clk4msk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. To check whether the Laplacian is symmetric or not we can compare it with its transpose."
      ],
      "metadata": {
        "id": "rXay919P9I1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.allclose(L,np.transpose(L))"
      ],
      "metadata": {
        "id": "1eMMsweS9LIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A matrix is said to be positive semidefinite when all its eigenvalues are non-negative. We can check that after we get rid of the numerical artifacts. To be on the safe side I added small positive number to the eigenvalue matrix before checking."
      ],
      "metadata": {
        "id": "zaKf3PLJ-whD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w+1e-10 >= 0"
      ],
      "metadata": {
        "id": "d8nujdZi_YSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Since it is arbitrary to choose the start and the end of the directed graph, we can multiply upper triangle with -1 and check the hypothesis."
      ],
      "metadata": {
        "id": "QteNmAWWnMTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = to_networkx(data, to_undirected=False)"
      ],
      "metadata": {
        "id": "HXVsloioHovC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B = nx.to_numpy_array(G)\n",
        "L == np.matmul(B,np.transpose(B))"
      ],
      "metadata": {
        "id": "-phrm29_Ax43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa = nx.to_numpy_array(G)\n",
        "aa"
      ],
      "metadata": {
        "id": "02V7ef92n2Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bb = aa\n",
        "for ii in range(np.size(bb,0)):\n",
        "  for jj in range(ii, np.size(bb,1)-1):\n",
        "    bb[ii,jj+1] = bb[ii,jj+1]*-1\n",
        "bb"
      ],
      "metadata": {
        "id": "I2RJSGQ4GKmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L == np.matmul(bb,np.transpose(bb))"
      ],
      "metadata": {
        "id": "TyK-cfmDFZYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried direct calculation or assigning in and out edges but could not show that last equality works."
      ],
      "metadata": {
        "id": "jabz_7nxJU_T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0d-zG2Noh0O"
      },
      "source": [
        "# Acknowledgements\n",
        "\n",
        "An intial version of this Laboratory (in Spanish) was conceived and developed by colleagues from [Facultad de Ingenieria](https://www.fing.edu.uy) in Montevideo, Uruguay and myself, for the course **[Aprendizaje Autom√°tico para Datos en Grafos](https://eva.fing.edu.uy/course/view.php?id=1626&section=0)**.\n",
        "\n",
        "The first part in the section '**Introduction to PyTorch Geometric**' is based on [this notebook](https://colab.research.google.com/drive/16tqEHKOLUgYvXKx1V3blfYGpQb1_09MG?usp=sharing#scrollTo=bbny-iTO7NQN) from Stanford's course **[CS224W: Machine Learning with Graphs](http://web.stanford.edu/class/cs224w)**."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}