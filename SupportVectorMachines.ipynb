{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZCWXeJQB6g6zX/+17dLGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yavuzuzun/projects/blob/main/SupportVectorMachines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression analysis. The main idea behind SVM is to find a hyperplane that separates the data points into different classes with maximum margin. \n",
        "\n",
        "In simple terms, SVM tries to find a decision boundary that maximizes the margin between the two classes of data. The margin is defined as the distance between the decision boundary and the closest points of each class. The points that lie on the margin are called support vectors.\n",
        "\n",
        "The SVM algorithm works as follows:\n",
        "\n",
        "1. Data Preparation: SVM algorithm starts with the preparation of data. It takes labeled data as input and classifies the data into different classes based on the input features.\n",
        "\n",
        "2. Selecting the Kernel Function: The next step is to select the kernel function. Kernel function maps the input data into a higher-dimensional space where the data can be easily classified. The most commonly used kernel functions are linear, polynomial, and radial basis function (RBF) kernel.\n",
        "\n",
        "3. Optimization of Hyperparameters: SVM algorithm requires the optimization of hyperparameters to improve the accuracy of the classification. The hyperparameters include the regularization parameter (C) and kernel parameters.\n",
        "\n",
        "4. Training the SVM Model: After the selection of kernel function and optimization of hyperparameters, the SVM model is trained using the labeled data. The SVM algorithm tries to find the optimal hyperplane that separates the data into different classes.\n",
        "\n",
        "5. Prediction: Once the SVM model is trained, it can be used to predict the class of new data points.\n",
        "\n",
        "The main advantages of SVM algorithm are its ability to handle non-linearly separable data, high accuracy, and ability to work well with high-dimensional data. However, SVM can be computationally expensive and sensitive to the choice of kernel function and hyperparameters."
      ],
      "metadata": {
        "id": "J0Ez-GNHufEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Prepackage\n",
        "### Example: Classifying Breast Cancer using SVMs\n",
        "In this example, we'll use SVMs to classify breast cancer as either benign or malignant based on several features, including the radius of the tumor, texture, and more.\n",
        "\n",
        "### Step 1: Load and preprocess the data\n",
        "First, we need to load and preprocess the data. We'll use the load_breast_cancer function from scikit-learn to load the data, and then split it into training and testing sets."
      ],
      "metadata": {
        "id": "qvHaf5h-P3B_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ci17iTF0ueaA"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)\n",
        "\n",
        "# Scale the data to improve SVM performance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we're scaling the data using the StandardScaler function, which subtracts the mean and divides by the standard deviation of each feature. This is often done to improve the performance of SVMs.\n",
        "\n",
        "### Step 2: Train and evaluate the SVM\n",
        "Next, we'll create an SVM model using the svm.SVC function, and train it on the training data. We'll use the radial basis function (RBF) kernel, which is a common choice for SVMs."
      ],
      "metadata": {
        "id": "U9csPpCxQI48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# Create an SVM classifier with an RBF kernel\n",
        "clf = svm.SVC(kernel='rbf')\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier on the testing data\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuHKLu4lQK8G",
        "outputId": "945c7c5b-24d9-4d94-e414-2712d69e30dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.965034965034965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we run this code, we get an accuracy of around 0.97, which is quite good. However, there are some limitations and trade-offs to consider when using SVMs.\n",
        "\n",
        "### Strengths of SVMs\n",
        "  - SVMs can perform well even when the number of features is much larger than the number of samples.\n",
        "\n",
        "  - SVMs can handle non-linear decision boundaries by using different kernel functions.\n",
        "\n",
        "  - SVMs are less prone to overfitting than other classification algorithms, as long as the kernel and other parameters are chosen carefully.\n",
        "\n",
        "  - SVMs can work well even with small datasets, as long as the number of features is not too large.\n",
        "\n",
        "### Weaknesses of SVMs\n",
        "  - SVMs can be computationally expensive to train, especially if the number of features or samples is very large.\n",
        "\n",
        "  - SVMs can be sensitive to the choice of kernel function and other hyperparameters, which can be difficult to tune without a lot of experimentation.\n",
        "\n",
        "  - SVMs do not provide probability estimates by default, which can make it difficult to interpret the results.\n",
        "\n",
        "  - SVMs can be sensitive to outliers in the data, which can affect the decision boundary. This can be mitigated somewhat by using a more robust kernel function, such as the RBF kernel."
      ],
      "metadata": {
        "id": "-8NQG-cVQSPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Scratch"
      ],
      "metadata": {
        "id": "AzfLllVzQvJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, lr=0.01, C=1.0, max_iters=1000, tol=1e-3):\n",
        "        self.lr = lr\n",
        "        self.C = C\n",
        "        self.max_iters = max_iters\n",
        "        self.tol = tol\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        # Initialize the parameters\n",
        "        m, n = X.shape\n",
        "        self.w = np.zeros(n)\n",
        "        self.b = 0\n",
        "        self.support_vectors = None\n",
        "        \n",
        "        # Iterate until convergence or max_iters\n",
        "        for _ in range(self.max_iters):\n",
        "            # Compute the margin\n",
        "            margin = y * (X.dot(self.w) + self.b)\n",
        "            \n",
        "            # Identify the support vectors (points that violate the margin)\n",
        "            idx = np.where(margin <= 1)[0]\n",
        "            support_vectors = X[idx]\n",
        "            support_labels = y[idx]\n",
        "            \n",
        "            # Compute the gradient of the loss function\n",
        "            grad_w = self.w - self.C * np.sum(support_labels[:, np.newaxis] * support_vectors, axis=0)\n",
        "            grad_b = -self.C * np.sum(support_labels)\n",
        "            \n",
        "            # Update the parameters\n",
        "            self.w -= self.lr * grad_w\n",
        "            self.b -= self.lr * grad_b\n",
        "            \n",
        "            # Check for convergence\n",
        "            if np.linalg.norm(grad_w) < self.tol:\n",
        "                self.support_vectors = support_vectors\n",
        "                break\n",
        "        \n",
        "        if self.support_vectors is None:\n",
        "            self.support_vectors = X[idx]\n",
        "            \n",
        "    def predict(self, X):\n",
        "        # Compute the predicted class labels for a set of inputs X\n",
        "        return np.sign(X.dot(self.w) + self.b)\n"
      ],
      "metadata": {
        "id": "zPz5VD1qQyqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The SVM class is initialized with several hyperparameters, including the learning rate lr, the regularization parameter C, the maximum number of iterations max_iters, and the tolerance tol.\n",
        "  - The fit method takes in a matrix X of training examples (one example per row) and a vector y of class labels (-1 or 1). It uses the training examples to learn the parameters of the SVM (the weight vector w and the bias b) using gradient descent.\n",
        "  - The predict method takes in a matrix X of test examples and returns a vector of predicted class labels (-1 or 1).\n",
        "\n",
        "Note that this implementation uses the hinge loss function as the loss function for the SVM, and uses gradient descent to optimize the parameters. It also includes a regularization term to prevent overfitting, and uses the norm of the gradient as a stopping criterion to check for convergence.\n",
        "\n",
        "### SVM using the dual problem"
      ],
      "metadata": {
        "id": "6RyeApCCSQeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, C=1.0):\n",
        "        self.C = C\n",
        "        self.alpha = None\n",
        "        self.support_vectors = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Define the objective function for the dual problem\n",
        "        def objective(alpha):\n",
        "            return 0.5 * np.dot(alpha, alpha * y) - np.sum(alpha)\n",
        "\n",
        "        # Define the constraints for the dual problem\n",
        "        def constraints(alpha):\n",
        "            return np.dot(alpha, y)\n",
        "\n",
        "        # Set up the optimization problem for the dual problem\n",
        "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
        "        cons = {'type': 'eq', 'fun': constraints}\n",
        "        res = minimize(objective, np.zeros(n_samples), bounds=bounds, constraints=cons)\n",
        "\n",
        "        # Get the optimal values of the dual variables\n",
        "        self.alpha = res.x\n",
        "\n",
        "        # Identify the support vectors\n",
        "        idx = self.alpha > 1e-5\n",
        "        self.support_vectors = X[idx]\n",
        "        self.alpha = self.alpha[idx]\n",
        "        self.y = y[idx]\n",
        "\n",
        "        # Compute the bias term\n",
        "        self.bias = np.mean(self.y - np.sum(self.alpha * self.y * np.dot(self.support_vectors, X.T), axis=0))\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute the predicted class labels for a set of inputs X\n",
        "        y_pred = np.sum(self.alpha * self.y * np.dot(self.support_vectors, X.T), axis=0) + self.bias\n",
        "        y_pred = np.sign(y_pred)\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "QJjXt1k0SQAH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}