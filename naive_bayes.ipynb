{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS1FNmXSQa3U2t1dz5SNPi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yavuzuzun/projects/blob/main/naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is naive Bayes\n",
        "\n",
        "Naive Bayes is a probabilistic machine learning algorithm that is commonly used for classification tasks. It is based on Bayes' theorem, which is a fundamental concept in probability theory.\n",
        "\n",
        "Naive Bayes assumes that the input features are conditionally independent given the class label. This assumption simplifies the probability calculation and makes the model computationally efficient. The model estimates the conditional probability of the class label given the input features using Bayes' theorem:\n",
        "\n",
        "P(y|x1, x2, ..., xn) = P(x1, x2, ..., xn|y) * P(y) / P(x1, x2, ..., xn)\n",
        "\n",
        "where y is the class label, and x1, x2, ..., xn are the input features. The model calculates the likelihood P(x1, x2, ..., xn|y) and the prior probability P(y) from the training data. The evidence probability P(x1, x2, ..., xn) is a normalizing constant that ensures that the probabilities sum up to 1.\n",
        "\n",
        "Naive Bayes is called \"naive\" because it assumes that the input features are conditionally independent, which is often not true in practice. However, despite this simplifying assumption, Naive Bayes can be surprisingly effective in many real-world applications, especially in text classification and spam filtering."
      ],
      "metadata": {
        "id": "lhLoIHQ4fs4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With prepackage"
      ],
      "metadata": {
        "id": "ltYkxI7VewxL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m1tBcBuaer8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98eb8a8-1191-407b-f49e-3ac770669317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7728359001593202\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.79      0.77      0.78       319\n",
            "           comp.graphics       0.67      0.74      0.70       389\n",
            " comp.os.ms-windows.misc       0.20      0.00      0.01       394\n",
            "comp.sys.ibm.pc.hardware       0.56      0.77      0.65       392\n",
            "   comp.sys.mac.hardware       0.84      0.75      0.79       385\n",
            "          comp.windows.x       0.65      0.84      0.73       395\n",
            "            misc.forsale       0.93      0.65      0.77       390\n",
            "               rec.autos       0.87      0.91      0.89       396\n",
            "         rec.motorcycles       0.96      0.92      0.94       398\n",
            "      rec.sport.baseball       0.96      0.87      0.91       397\n",
            "        rec.sport.hockey       0.93      0.96      0.95       399\n",
            "               sci.crypt       0.67      0.95      0.78       396\n",
            "         sci.electronics       0.79      0.66      0.72       393\n",
            "                 sci.med       0.87      0.82      0.85       396\n",
            "               sci.space       0.83      0.89      0.86       394\n",
            "  soc.religion.christian       0.70      0.96      0.81       398\n",
            "      talk.politics.guns       0.69      0.91      0.79       364\n",
            "   talk.politics.mideast       0.85      0.94      0.89       376\n",
            "      talk.politics.misc       0.58      0.63      0.60       310\n",
            "      talk.religion.misc       0.89      0.33      0.49       251\n",
            "\n",
            "                accuracy                           0.77      7532\n",
            "               macro avg       0.76      0.76      0.75      7532\n",
            "            weighted avg       0.76      0.77      0.75      7532\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "# Convert the raw text data to numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier on the training data\n",
        "y_train = newsgroups_train.target\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Use the classifier to predict the classes of the test data\n",
        "y_test = newsgroups_test.target\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print a classification report with precision, recall, and F1-score for each class\n",
        "class_names = newsgroups_train.target_names\n",
        "report = classification_report(y_test, y_pred, target_names=class_names)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From scratch"
      ],
      "metadata": {
        "id": "pBXT_aIne1vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll create a toy dataset. Let's say we have a dataset of emails and we want to classify them as either spam or not spam based on the words in the email. Here's what our dataset might look like:"
      ],
      "metadata": {
        "id": "bAI9iDmFe7Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emails = [\n",
        "    ('Hey there! I thought you might find this interesting. Click here.', 'spam'),\n",
        "    ('Get viagra for a discount price. Limited time offer.', 'spam'),\n",
        "    ('URGENT: Your help is needed to secure your account!', 'spam'),\n",
        "    ('Hi, just wanted to check in and see how you were doing.', 'not spam'),\n",
        "    ('Reminder: Meeting tomorrow at 2pm.', 'not spam'),\n",
        "    ('Please submit your expense reports by Friday.', 'not spam')\n",
        "]\n"
      ],
      "metadata": {
        "id": "0En3BPeye3xV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll need to preprocess the data by creating a vocabulary of all the unique words in the dataset and creating a bag of words representation for each email. We can do this using the CountVectorizer class from scikit-learn:"
      ],
      "metadata": {
        "id": "54Q5aMx3e_d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create the vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# fit the vectorizer on the emails\n",
        "corpus = [email[0] for email in emails]\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# create a bag of words representation for each email\n",
        "X = vectorizer.transform(corpus).toarray()\n",
        "\n",
        "# create the target vector\n",
        "y = [email[1] for email in emails]\n"
      ],
      "metadata": {
        "id": "VlwfuHuUe-70"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll split the data into training and testing sets:"
      ],
      "metadata": {
        "id": "q-VzbEgle-kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "xWAXw3D_e-bG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we're ready to implement the Naive Bayes algorithm. First, we'll need to calculate the prior probabilities of each class:"
      ],
      "metadata": {
        "id": "wF4FwvQ5e-R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# calculate the prior probabilities of each class\n",
        "classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "prior_probs = class_counts / len(y_train)\n"
      ],
      "metadata": {
        "id": "BbHVD96ze-J3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll calculate the likelihoods of each word given each class using Laplace smoothing:"
      ],
      "metadata": {
        "id": "JnHhXkeGe-Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the likelihoods of each word given each class\n",
        "word_counts = np.zeros((len(classes), X_train.shape[1]))\n",
        "for i, c in enumerate(classes):\n",
        "    X_c = X_train[y_train == c]\n",
        "    if len(X_c) > 0:\n",
        "      word_counts[i, :] = X_c.sum(axis=0) + 1\n",
        "\n",
        "word_probs = word_counts / word_counts.sum(axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "_lmvl7uie9xO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00dd9418-da46-4b81-912f-be4af2721aba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-16dcdc9be0f1>:8: RuntimeWarning: invalid value encountered in true_divide\n",
            "  word_probs = word_counts / word_counts.sum(axis=1, keepdims=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can make predictions on the test set:"
      ],
      "metadata": {
        "id": "oEFM0TNOe9gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on the test set\n",
        "y_pred = []\n",
        "for x in X_test:\n",
        "    class_probs = prior_probs.copy()\n",
        "    for i, p in enumerate(class_probs):\n",
        "        for j, xj in enumerate(x):\n",
        "            class_probs[i] *= word_probs[i, j] ** xj\n",
        "    y_pred.append(classes[np.argmax(class_probs)])\n",
        "\n",
        "# evaluate the performance of the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "jy1jY1pEe762",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af5f221-fcd4-403f-eb47-7d05cce5cbc9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I still need to work on it..."
      ],
      "metadata": {
        "id": "2deTTNt0OnDY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-HoxllkOwlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}